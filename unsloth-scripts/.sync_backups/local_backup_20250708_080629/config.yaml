# Qwen Fine-tuning Configuration with Unsloth

# Model Configuration
model:
  model_name: "unsloth/Qwen3-14B"  # Options: Qwen3-4B, Qwen3-8B, Qwen3-14B, Qwen3-32B
  max_seq_length: 2048  # Sequence length (adjust based on your needs)
  dtype: null  # Auto-detect best dtype
  load_in_4bit: true  # Use 4-bit quantization for memory efficiency
  device_map: "auto"  # Automatic device mapping
  
  # LoRA Configuration
  lora_r: 16  # LoRA rank
  lora_alpha: 16  # LoRA alpha
  lora_dropout: 0.1  # LoRA dropout
  lora_targets:  # Target modules for LoRA
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  use_rslora: false  # Use RSLoRA
  random_state: 42  # Random seed for reproducibility
  loftq_config: null  # LoftQ configuration

# Dataset Configuration
dataset:
  type: "local"  # Options: "huggingface", "local"
  name: "mixed_dataset"  # Hugging Face dataset name (if type is huggingface)
  path: "mixed_dataset.json"  # Local dataset path (if type is local)
  split: "train"  # Dataset split to use
  text_column: "conversation"  # Column name containing conversations
  use_chat_template: true  # Apply chat template formatting

# Training Configuration
training:
  # Batch size and accumulation
  per_device_train_batch_size: 2  # Batch size per device
  gradient_accumulation_steps: 4  # Gradient accumulation steps
  
  # Learning rate and schedule
  learning_rate: 2e-4  # Learning rate
  lr_scheduler_type: "linear"  # Learning rate scheduler
  warmup_steps: 5  # Warmup steps
  weight_decay: 0.01  # Weight decay
  
  # Training duration
  num_train_epochs: 3  # Number of training epochs
  max_steps: -1  # Maximum training steps (-1 for full epochs)
  
  # Precision and optimization
  fp16: false  # Use FP16 (auto-detected based on GPU)
  bf16: true  # Use BF16 (auto-detected based on GPU)
  optimizer: "adamw_8bit"  # Optimizer
  
  # Logging and saving
  logging_steps: 1  # Logging frequency
  save_steps: 100  # Model save frequency
  save_total_limit: 3  # Maximum number of saved checkpoints
  
  # Output directory
  output_dir: "./qwen_finetuned"  # Output directory for model and checkpoints
  
  # Data processing
  dataset_num_proc: 2  # Number of processes for dataset processing
  dataloader_num_workers: 0  # Number of dataloader workers
  packing: false  # Pack sequences for efficiency
  
  # Reporting
  report_to: ["wandb"]  # Reporting services (wandb, tensorboard, etc.)
  
  # Model saving format
  save_method: "merged_16bit"  # Options: "lora", "merged_16bit", "merged_4bit"
  
  # Reproducibility
  seed: 42  # Random seed

# Weights & Biases Configuration
use_wandb: false  # Enable W&B logging
wandb:
  project: "qwen-finetuning"  # W&B project name
  run_name: "qwen3-14b-mixed-dataset"  # W&B run name
  tags: ["qwen", "unsloth", "lora"]  # W&B tags

# Hardware-specific configurations
hardware:
  # For different GPU memory configurations
  gpu_16gb:
    model_name: "unsloth/Qwen3-14B"
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 4
    max_seq_length: 2048
  
  gpu_24gb:
    model_name: "unsloth/Qwen3-14B"
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 2
    max_seq_length: 4096
  
  gpu_40gb:
    model_name: "unsloth/Qwen3-32B"
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 4
    max_seq_length: 4096

# Evaluation Configuration (optional)
evaluation:
  eval_steps: 100  # Evaluation frequency
  eval_dataset: null  # Evaluation dataset path
  metric_for_best_model: "loss"  # Metric to track for best model
  load_best_model_at_end: true  # Load best model at end of training