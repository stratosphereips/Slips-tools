{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen Model Fine-tuning with Unsloth\n",
    "\n",
    "This notebook demonstrates how to fine-tune a Qwen model using the Unsloth framework for efficient training.\n",
    "\n",
    "## Features\n",
    "- 2x faster training with 70% less VRAM usage\n",
    "- Support for 4-bit quantization and LoRA adapters\n",
    "- Mixed dataset training (reasoning + conversational)\n",
    "- Compatible with Google Colab and local environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "!pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers datasets accelerate peft trl\n",
    "!pip install bitsandbytes xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = \"unsloth/Qwen3-14B\"  # Choose based on your GPU memory\n",
    "max_seq_length = 2048  # Adjust based on your needs\n",
    "dtype = None  # Auto-detect best dtype\n",
    "load_in_4bit = True  # Use 4-bit quantization for memory efficiency\n",
    "\n",
    "# LoRA configuration\n",
    "lora_r = 16\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")\n",
    "print(f\"4-bit quantization: {load_in_4bit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer with Unsloth optimizations\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters for efficient fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_r,\n",
    "    target_modules=target_modules,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters added successfully!\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset (you can replace this with your own)\n",
    "# Using Alpaca dataset as an example\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")  # Small subset for demo\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"Sample keys: {dataset.column_names}\")\n",
    "\n",
    "# Show a sample\n",
    "sample = dataset[0]\n",
    "print(\"\\nSample data:\")\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {value[:100]}...\" if len(str(value)) > 100 else f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Dataset for Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset for chat template\n",
    "def format_chat_template(examples):\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(\n",
    "        examples[\"instruction\"], examples[\"input\"], examples[\"output\"]\n",
    "    ):\n",
    "        # Create conversation format\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": instruction + (f\"\\n\\n{input_text}\" if input_text else \"\")},\n",
    "            {\"role\": \"assistant\", \"content\": output}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(format_chat_template, batched=True)\n",
    "\n",
    "print(\"Dataset formatted successfully!\")\n",
    "print(f\"Sample formatted text:\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    num_train_epochs=1,  # Reduced for demo\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    output_dir=\"./qwen_finetuned\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"- Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"- Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"- Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"- Mixed precision: {'BF16' if training_args.bf16 else 'FP16' if training_args.fp16 else 'FP32'}\")\n",
    "print(f\"- Epochs: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SFT trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")\n",
    "print(f\"Training dataset size: {len(trainer.train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./qwen_finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "# Optional: Save in different formats\n",
    "print(\"\\nSaving model in merged 16-bit format...\")\n",
    "model.save_pretrained_merged(\n",
    "    output_dir + \"_merged_16bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\"\n",
    ")\n",
    "\n",
    "print(\"Model saved in multiple formats!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "# Test prompt\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain what machine learning is in simple terms.\"}\n",
    "]\n",
    "\n",
    "test_prompt = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Test prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated response:\")\n",
    "print(response[len(test_prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display memory usage statistics\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Usage:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Max allocated: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nMemory cache cleared.\")\n",
    "else:\n",
    "    print(\"No GPU available - running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Efficient Model Loading**: Using Unsloth's optimized model loading with 4-bit quantization\n",
    "2. **LoRA Fine-tuning**: Adding Low-Rank Adaptation layers for efficient parameter updates\n",
    "3. **Dataset Formatting**: Converting datasets to chat template format\n",
    "4. **Optimized Training**: Using Unsloth's SFT trainer with memory-efficient settings\n",
    "5. **Model Saving**: Saving in multiple formats for different use cases\n",
    "6. **Inference Testing**: Testing the fine-tuned model with faster inference\n",
    "\n",
    "### Key Benefits of Unsloth:\n",
    "- **2x faster training** compared to standard methods\n",
    "- **70% less VRAM usage** enabling larger models on smaller GPUs\n",
    "- **No accuracy loss** - maintains full model quality\n",
    "- **Easy integration** with existing workflows\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different model sizes (4B, 8B, 14B, 32B)\n",
    "2. Try different datasets and mixing ratios\n",
    "3. Adjust LoRA parameters for your specific use case\n",
    "4. Implement evaluation metrics for your domain\n",
    "5. Deploy the model using your preferred serving framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}