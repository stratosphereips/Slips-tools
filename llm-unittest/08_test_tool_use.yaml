description: Generate Function Call from Prompt

providers:
  - id: openai:chat:smollm2:1.7b
    config:
      num_predict: 2048
  - id: openai:chat:gemma3:1b
    config:
      num_predict: 2048
  - id: openai:chat:llama3.2:1b
    config:
      num_predict: 2048
  - id: openai:chat:llama3.2:3b
    config:
      num_predict: 2048
  - id: openai:chat:phi4-mini:latest
    config:
      num_predict: 2048
  - id: openai:chat:qwen2.5:1.5b
    config:
      num_predict: 2048
  - id: openai:chat:qwen2.5:3b
    config:
      num_predict: 2048
  - id: openai:chat:granite3.1-dense:2b
    config:
      num_predict: 2048
prompts:
  - file://prompts/tool_use_personality.json
tests:
  - vars:
      city: "Mendoza City"
    assert:
      - type: contains
        value: '<function_call>[{"function": {"name": "get_current_weather", "arguments": {"location": "Mendoza City", "unit": "celsius"}}}]</function_call>'
  - vars:
      city: "Mendoza City"
    assert:
      - type: llm-rubric
        value: "Has the answer included the function call with the correct name and parameters?. The answer should include the location and unit parameters in correct JSON" 


defaultTest:
  options:
    provider: openai:chat:qwen2.5-coder:latest
